This section illustrates how to do approximate topic modeling in Python. We will
use a technique called non-negative matrix factorization (NMF) that
strongly resembles Latent Dirichlet Allocation (LDA) which we covered in the
previous section, Topic modeling with MALLET. [1] Whereas LDA is
a probabilistic model capable of expressing uncertainty about the placement of
topics across texts and the assignment of words to topics, NMF is
a deterministic algorithm which arrives at a single representation of the
corpus. For this reason, NMF is often characterized as a machine learning
algorithm. Like LDA, NMF arrives at its representation of a corpus in terms of
something resembling “latent topics”.
Note
The name “Non-negative matrix factorization” has the virtue of being
transparent. A “non-negative matrix” is a matrix containing non-negative
values (here zero or positive word frequencies). And
factorization refers to the familiar kind of mathematical factorization.
Just as a polynomial \(x^2 + 3x + 2\) may be factored into a simple
product \((x+2)(x+1)\), so too may a matrix
\(\bigl(\begin{smallmatrix} 6&2&4\\ 9&3&6 \end{smallmatrix} \bigr)\) be
factored into the product of two smaller matrices
\(\bigl(\begin{smallmatrix} 2\\ 3 \end{smallmatrix} \bigr)
\bigl(\begin{smallmatrix} 3&2&1 \end{smallmatrix} \bigr)\).
This section follows the procedures described in Topic modeling with MALLET,
making the substitution of NMF for LDA where appropriate.
This section uses the novels by Brontë and Austen. These novels are divided into
parts as follows:
As always we need to give Python access to our corpus. In this case we will work
with our familiar document-term matrix.
By analogy with LDA, we will use NMF to get a document-topic matrix (topics here
will also be referred to as “components”) and a list of top words for each
topic. We will make analogy clear by using the same variable names:
doctopic and topic_words
To make the analysis and visualization of NMF components similar to that of
LDA’s topic proportions, we will scale the document-component matrix such that
the component values associated with each document sum to one.
Now we will average those topic shares associated with the same novel together
— just as we did with the topic shares from MALLET.
In order to fit into the space available, the table above displays the first 15
of 20 topics.
The topics (or components) of the NMF fit preserve the distances between novels (see the figures below).
Even though the NMF fit “discards” the fine-grained detail recorded in the
matrix of word frequencies, the matrix factorization performed allows us to
reconstruct the salient details of the underlying matrix.
As we did in the previous section, let us identify the most significant topics
for each text in the corpus.  This procedure does not differ in essence from the
procedure for identifying the most frequent words in each text.
And we already have lists of words (topic_words) most strongly associated
with the components. For reference, we will display them again:
There are many ways to inspect and to visualize topic models. Some of the most
common methods are covered in Visualizing topic models.
Consider the task of finding the topics that are distinctive of Austen using the
NMF “topics”. Using the simple difference-in-averages we can find topics that to
be associated with Austen’s novels rather than Brontë’s.
All materials are published under a Creative Commons Attribution 4.0 International license (CC-BY 4.0).
Comments are welcome, as are reports of bugs and typos. Please use the project’s issue tracker.
These tutorials have been developed with support from the DARIAH-DE initiative, the German branch of DARIAH-EU, the European Digital Research Infrastructure for the Arts and Humanities consortium. Funding has been provided by the German Federal Ministry for Research and Education (BMBF) under the identifier 01UG1110J.
